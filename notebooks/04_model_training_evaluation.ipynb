{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf974b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train rows 6483713 | Test rows 1620945\n",
      "===== TRAINING RandomForest =====\n",
      "RandomForest: AUC = 0.8639 | train_time = 2660.2s\n",
      "===== TRAINING LogisticRegression =====\n",
      "LogisticRegression: AUC = 0.8719 | train_time = 167.0s\n",
      "===== TRAINING GBT =====\n",
      "GBT: AUC = 0.8805 | train_time = 6429.2s\n",
      "\n",
      "ğŸ† Melhor modelo: GBT AUC 0.8804897283812273\n"
     ]
    }
   ],
   "source": [
    "# 04_model_training_evaluation.ipynb\n",
    "\"\"\"\n",
    "Treino e avaliaÃ§Ã£o de modelos â€“ Chicago Crime (arrest prediction)\n",
    "* Logs detalhados sÃ£o impressos no stdout\n",
    "* Resumo final em tabela pandas para fÃ¡cil copyâ€‘paste no relatÃ³rio\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import PipelineModel\n",
    "from pyspark.ml.classification import (\n",
    "    LogisticRegression, RandomForestClassifier, GBTClassifier\n",
    ")\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "import time, json, os, pandas as pd\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "         .appName(\"Crime_Model_Train\")\n",
    "         .config(\"spark.sql.shuffle.partitions\", \"200\")  # mantemos baixo para cluster local\n",
    "         .getOrCreate())\n",
    "\n",
    "DATA_PATH  = \"../dados/chicago_ready.parquet\"\n",
    "MODEL_DIR  = \"../dados/best_model\"\n",
    "LOG_PATH   = \"../dados/train_logs.json\"\n",
    "\n",
    "print(\"â‰¡â‰¡ Loading dataset:\", DATA_PATH)\n",
    "\n",
    "df = spark.read.parquet(DATA_PATH)\n",
    "print(\"â†’ Rows:\", df.count())\n",
    "print(\"â†’ Schema:\")\n",
    "df.printSchema()\n",
    "\n",
    "train, test = df.randomSplit([0.8, 0.2], seed=42)\n",
    "print(f\"ğŸš‚ train rows={train.count()}  ğŸ§ª test rows={test.count()}\")\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"Arrest\", metricName=\"areaUnderROC\")\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ modelos & grelhas -----------------------------------\n",
    "lr  = LogisticRegression(labelCol=\"Arrest\", featuresCol=\"features\", maxIter=10)\n",
    "rf  = RandomForestClassifier(labelCol=\"Arrest\", featuresCol=\"features\")\n",
    "gbt = GBTClassifier(labelCol=\"Arrest\", featuresCol=\"features\", maxIter=20)\n",
    "\n",
    "lr_grid = (ParamGridBuilder()\n",
    "           .addGrid(lr.regParam, [0.0, 0.1])\n",
    "           .addGrid(lr.elasticNetParam, [0.0, 0.5])\n",
    "           .build())\n",
    "\n",
    "rf_grid = (ParamGridBuilder()\n",
    "           .addGrid(rf.numTrees, [50])              # grelha enxuta\n",
    "           .addGrid(rf.maxDepth, [5, 10])\n",
    "           .build())\n",
    "\n",
    "gbt_grid = (ParamGridBuilder()\n",
    "            .addGrid(gbt.maxDepth, [5, 8])\n",
    "            .addGrid(gbt.stepSize, [0.1])\n",
    "            .build())\n",
    "\n",
    "def build_cv(model, grid):\n",
    "    return CrossValidator(\n",
    "        estimator=model,\n",
    "        estimatorParamMaps=grid,\n",
    "        evaluator=evaluator,\n",
    "        numFolds=3,\n",
    "        seed=42,\n",
    "        parallelism=1,           # mantemos baixo p/ evitar OOM\n",
    "        collectSubModels=False,\n",
    "    )\n",
    "\n",
    "models = {\n",
    "    \"LogisticRegression\": build_cv(lr, lr_grid),\n",
    "    \"RandomForest\":       build_cv(rf, rf_grid),\n",
    "    \"GBT\":               build_cv(gbt, gbt_grid)\n",
    "}\n",
    "\n",
    "results = []\n",
    "print(\"\\n===== TRAINING START =====\")\n",
    "for name, cv in models.items():\n",
    "    try:\n",
    "        print(f\"\\nâš™ï¸  Treinando {name} â€¦\")\n",
    "        t0 = time.time()\n",
    "        cv_model = cv.fit(train)\n",
    "        train_time = time.time() - t0\n",
    "        auc = evaluator.evaluate(cv_model.transform(test))\n",
    "\n",
    "        # Confusion matrix\n",
    "        preds = cv_model.transform(test).select(\"prediction\", \"Arrest\")\n",
    "        metrics = MulticlassMetrics(preds.rdd.map(lambda r: (r[0], r[1])))\n",
    "        cm = metrics.confusionMatrix().toArray().tolist()\n",
    "\n",
    "        print(f\"{name}: AUC = {auc:.4f} | train_time = {train_time:.1f}s\")\n",
    "\n",
    "        # salvar modelo individual (path dedicado)\n",
    "        m_path = f\"../dados/{name.replace(' ', '_')}_model\"\n",
    "        cv_model.bestModel.write().overwrite().save(m_path)\n",
    "\n",
    "        results.append({\n",
    "            \"model\": name,\n",
    "            \"auc\": auc,\n",
    "            \"train_time_s\": train_time,\n",
    "            \"confusion_matrix\": cm,\n",
    "            \"model_path\": m_path,\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ {name} falhou: {e}\")\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ melhor & persistÃªncia -----------------------------\n",
    "if not results:\n",
    "    raise RuntimeError(\"Todos os modelos falharam â€” verificar logs.\")\n",
    "\n",
    "best = max(results, key=lambda d: d[\"auc\"])\n",
    "print(\"\\nğŸ† Melhor modelo:\", best[\"model\"], \"AUC\", best[\"auc\"])\n",
    "\n",
    "spark.read.load(best[\"model_path\"]).write().overwrite().save(MODEL_DIR)\n",
    "print(\"Modelo salvo em\", MODEL_DIR)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Tabela resumo --------------------------------------\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nResumo:\")\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# salvar json para debug future\n",
    "with open(LOG_PATH, \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "print(\"ğŸ“ Logs JSON em\", LOG_PATH)\n",
    "\n",
    "spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vscode_pyspark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
