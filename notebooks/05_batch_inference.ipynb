{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd75e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 05_batch_inference.ipynb\n",
    "\"\"\"\n",
    "Batch inference – Chicago Crime\n",
    "* Usa o melhor modelo salvo em ../dados/best_model\n",
    "* Gera previsões num dataset completo ou em amostras de produção\n",
    "* Guarda saídas (parquet + csv) e métricas de qualidade\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import PipelineModel\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "import pandas as pd, json, os, time\n",
    "\n",
    "# ────────────────────────────────────────── config\n",
    "INPUT_PATH   = \"../dados/chicago_ready.parquet\"   # mesmo esquema que no treino\n",
    "MODEL_PATH   = \"../dados/best_model\"              # criado no Step 04\n",
    "PRED_PATH    = \"../dados/batch_preds.parquet\"\n",
    "CSV_SAMPLE   = \"../dados/batch_preds_sample.csv\"  # 1 % para quick‑look\n",
    "METRICS_JSON = \"../dados/batch_metrics.json\"\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "         .appName(\"Crime_Batch_Inference\")\n",
    "         .getOrCreate())\n",
    "\n",
    "print(\"≡≡ Carregando best model em\", MODEL_PATH)\n",
    "model = PipelineModel.load(MODEL_PATH)\n",
    "\n",
    "df_prod = spark.read.parquet(INPUT_PATH)\n",
    "print(\"→ Linhas para inferência:\", df_prod.count())\n",
    "\n",
    "# ────────────────────────────────────────── inferência\n",
    "print(\"⚙️  Aplicando modelo …\")\n",
    "t0 = time.time()\n",
    "pred = model.transform(df_prod)\n",
    "inf_time = time.time() - t0\n",
    "print(f\"✓ Inferência concluída em {inf_time:.1f}s\")\n",
    "\n",
    "pred.select(\"features\", \"prediction\", \"probability\", \"Arrest\").write.mode(\"overwrite\").parquet(PRED_PATH)\n",
    "print(\"📦 Previsões salvas em\", PRED_PATH)\n",
    "\n",
    "# 1 % sample CSV para análise manual\n",
    "(pred.select(\"prediction\", \"probability\", \"Arrest\")\n",
    "     .sample(0.01, seed=42)\n",
    "     .toPandas()\n",
    "     .to_csv(CSV_SAMPLE, index=False))\n",
    "print(\"📑 Amostra CSV →\", CSV_SAMPLE)\n",
    "\n",
    "# ────────────────────────────────────────── métricas\n",
    "print(\"📊 Métricas batch …\")\n",
    "\n",
    "binary_eval = BinaryClassificationEvaluator(labelCol=\"Arrest\", metricName=\"areaUnderROC\")\n",
    "auc = binary_eval.evaluate(pred)\n",
    "\n",
    "metrics = MulticlassMetrics(pred.select(\"prediction\", \"Arrest\").rdd.map(lambda r: (r[0], r[1])))\n",
    "cm = metrics.confusionMatrix().toArray().tolist()\n",
    "\n",
    "report = {\n",
    "    \"inference_rows\": df_prod.count(),\n",
    "    \"inference_time_s\": inf_time,\n",
    "    \"auc\": auc,\n",
    "    \"confusion_matrix\": cm,\n",
    "}\n",
    "print(json.dumps(report, indent=2))\n",
    "\n",
    "with open(METRICS_JSON, \"w\") as f:\n",
    "    json.dump(report, f, indent=2)\n",
    "print(\"📝 Métricas JSON em\", METRICS_JSON)\n",
    "\n",
    "spark.stop()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
