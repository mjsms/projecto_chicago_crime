{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa07c948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Pré‑processamento concluído – dados em ../dados/chicago_ready.parquet\n"
     ]
    }
   ],
   "source": [
    "# 03_preprocess_feature_engineering.ipynb\n",
    "\"\"\"\n",
    "Notebook 03 — Pré‑processamento & Feature Engineering\n",
    "Transforma o Chicago Crime bruto num dataset pronto para ML.\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import hour, dayofweek, to_timestamp, col\n",
    "from pyspark.ml.feature import (\n",
    "    StringIndexer, OneHotEncoder, VectorAssembler, Imputer\n",
    ")\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Crime_FE\").getOrCreate()\n",
    "\n",
    "INPUT_PATH  = \"../dados/chicago_crime.parquet\"\n",
    "OUTPUT_DATA = \"../dados/chicago_ready.parquet\"\n",
    "OUTPUT_PIPE = \"../dados/pipeline_fe\"\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "# 1. Ler dados brutos\n",
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "df = spark.read.parquet(INPUT_PATH)\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "# 2. Correções de timestamp e features temporais\n",
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "# Formato real: \"03/18/2015 12:00:00 AM\"  ➜ pattern \"MM/dd/yyyy hh:mm:ss a\"\n",
    "df = df.withColumn(\"Date\", to_timestamp(\"Date\", \"MM/dd/yyyy hh:mm:ss a\"))\n",
    "\n",
    "# Descarta registos sem data válida (muito poucos)\n",
    "df = df.filter(col(\"Date\").isNotNull())\n",
    "\n",
    "# Colunas derivadas\n",
    "df = (\n",
    "    df.withColumn(\"Hour\", hour(\"Date\"))        # 0‑23\n",
    "      .withColumn(\"DayOfWeek\", dayofweek(\"Date\")) # 1‑7 (Dom=1)\n",
    ")\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "# 3. Definir colunas para FE\n",
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "cat_cols  = [\"Primary Type\", \"Location Description\"]\n",
    "num_cols  = [\"Beat\", \"District\", \"Latitude\", \"Longitude\", \"Hour\", \"DayOfWeek\"]\n",
    "label_col = \"Arrest\"   # ainda booleano; convertemos depois para int\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "# 4. Pipeline de Feature Engineering\n",
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "indexers = [StringIndexer(inputCol=c, outputCol=f\"{c}_idx\", handleInvalid=\"keep\")\n",
    "            for c in cat_cols]\n",
    "encoders = [OneHotEncoder(inputCol=f\"{c}_idx\", outputCol=f\"{c}_vec\")\n",
    "            for c in cat_cols]\n",
    "\n",
    "# Imputação para colunas numéricas sujeitas a nulos\n",
    "num_imp_cols = [\"Beat\", \"District\", \"Latitude\", \"Longitude\"]\n",
    "\n",
    "imputer = Imputer(inputCols=num_imp_cols,\n",
    "                  outputCols=[f\"{c}_imp\" for c in num_imp_cols])\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[f\"{c}_vec\" for c in cat_cols] +\n",
    "              [f\"{c}_imp\" for c in num_imp_cols] + [\"Hour\", \"DayOfWeek\"],\n",
    "    outputCol=\"features\",\n",
    "    handleInvalid=\"keep\"  # evita falha se restar null\n",
    ")\n",
    "\n",
    "pipeline_fe = Pipeline(stages=indexers + encoders + [imputer, assembler])\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "# 5. Fit & Transform\n",
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "model_fe = pipeline_fe.fit(df)\n",
    "\n",
    "# Converte rótulo boolean → inteiro (1/0) e guarda como \"label\"\n",
    "df_ready = (\n",
    "    model_fe.transform(df)\n",
    "            .withColumn(\"label\", col(label_col).cast(\"int\"))\n",
    "            .select(\"features\", \"label\")\n",
    ")\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "# 6. Persistir dataset e pipeline\n",
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "df_ready.write.mode(\"overwrite\").parquet(OUTPUT_DATA)\n",
    "model_fe.write().overwrite().save(OUTPUT_PIPE)\n",
    "\n",
    "print(\"✅ Pré‑processamento concluído – dados em\", OUTPUT_DATA)\n",
    "\n",
    "spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vscode_pyspark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
