{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa07c948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "≡≡ Leitura parquet completo …\n",
      "+------+-------+\n",
      "|Arrest|  count|\n",
      "+------+-------+\n",
      "|  true| 673787|\n",
      "| false|2320665|\n",
      "+------+-------+\n",
      "\n",
      "⏳ Fit index/OHE/imputer/assembler no full …\n",
      "⏳ Fit PCA(k=30) em sample 20 % …\n",
      "✅ pipeline_fe salvo em ../dados/pipeline_fe\n",
      "⏳ Criando “READY_FULL” (features + label) para TODO o dataset …\n",
      "✅ parquet completo pronto em ../dados/chicago_ready_full.parquet  (2994452 linhas)\n",
      "⏳ Preparar parquet pequeno balanceado …\n",
      "✅ parquet pequeno salvo em ../dados/chicago_ready_small.parquet (150110 linhas)\n",
      "⌛ Elapsed: 91.4 s\n"
     ]
    }
   ],
   "source": [
    "# 03_preprocess_feature_engineering_fast.py\n",
    "\n",
    "import time\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.sql.functions import to_timestamp, hour, dayofweek, col\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.feature import (\n",
    "    StringIndexer, OneHotEncoder, Imputer,\n",
    "    VectorAssembler, PCA\n",
    ")\n",
    "\n",
    "# ────────────────────────── paths & params ────────────────────────────\n",
    "INPUT_FULL    = \"../dados/chicago_crime.parquet\"\n",
    "INPUT_SMALL   = \"../dados/chicago_crime_small.parquet\"\n",
    "PIPELINE_OUT  = \"../dados/pipeline_fe\"\n",
    "READY_SMALL   = \"../dados/chicago_ready_small.parquet\"\n",
    "READY_FULL    = \"../dados/chicago_ready_full.parquet\"  # <<< novo caminho\n",
    "PCA_K         = 30                                     # componentes principais\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"Preprocess_FE_fast\")\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"8\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "t0 = time.time()\n",
    "print(\"≡≡ Leitura parquet completo …\")\n",
    "df_full = spark.read.parquet(INPUT_FULL)\n",
    "\n",
    "# ────────────────────────── derivar colunas temporais ────────────────\n",
    "df_full = (\n",
    "    df_full\n",
    "    .withColumn(\"Date\", to_timestamp(\"Date\", \"MM/dd/yyyy HH:mm:ss a\"))\n",
    "    .filter(col(\"Date\").isNotNull())\n",
    "    .withColumn(\"Hour\",      hour(\"Date\"))\n",
    "    .withColumn(\"DayOfWeek\", dayofweek(\"Date\"))\n",
    ")\n",
    "\n",
    "df_full.groupBy(\"Arrest\").count().show()\n",
    "\n",
    "# ────────────────────────── listas de colunas ────────────────────────\n",
    "cat_cols = [\"Primary_Type\", \"Location_Description\"]\n",
    "num_cols = [\"Beat\", \"District\", \"Latitude\", \"Longitude\"]\n",
    "derived  = [\"Hour\", \"DayOfWeek\"]\n",
    "\n",
    "# ────────────────────────── estágios de FE ───────────────────────────\n",
    "indexers = [\n",
    "    StringIndexer(inputCol=c, outputCol=f\"{c}_idx\", handleInvalid=\"keep\")\n",
    "    for c in cat_cols\n",
    "]\n",
    "\n",
    "encoders = [\n",
    "    OneHotEncoder(inputCol=f\"{c}_idx\", outputCol=f\"{c}_vec\")\n",
    "    for c in cat_cols\n",
    "]\n",
    "\n",
    "imputer = Imputer(\n",
    "    inputCols=num_cols,\n",
    "    outputCols=[f\"{c}_imp\" for c in num_cols]\n",
    ")\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[f\"{c}_vec\" for c in cat_cols] +\n",
    "              [f\"{c}_imp\" for c in num_cols] +\n",
    "              derived,\n",
    "    outputCol=\"features\",\n",
    "    handleInvalid=\"keep\"\n",
    ")\n",
    "\n",
    "# ────────────────────────── 1) Fit de index/OHE/imputer/assembler no full ─────────\n",
    "print(\"⏳ Fit index/OHE/imputer/assembler no full …\")\n",
    "pre_pipe   = Pipeline(stages=indexers + encoders + [imputer, assembler])\n",
    "pre_model: PipelineModel = pre_pipe.fit(df_full)\n",
    "\n",
    "# ────────────────────────── 2) Fit PCA nos dados amostrados ──────────\n",
    "print(\"⏳ Fit PCA(k=30) em sample 20 % …\")\n",
    "sample_df  = pre_model.transform(df_full.sample(False, 0.20, 42))\n",
    "pca_model  = PCA(k=PCA_K, inputCol=\"features\", outputCol=\"pcaFeatures\").fit(sample_df)\n",
    "\n",
    "# ────────────────────────── 3) Concatena PipelineModel completo ─────────────────\n",
    "full_model = PipelineModel(stages=pre_model.stages + [pca_model])\n",
    "full_model.write().overwrite().save(PIPELINE_OUT)\n",
    "print(\"✅ pipeline_fe salvo em\", PIPELINE_OUT)\n",
    "\n",
    "# ────────────────────────── 4) Gerar parquet completo “READY_FULL” ─────────────\n",
    "print(\"⏳ Criando “READY_FULL” (features + label) para TODO o dataset …\")\n",
    "ready_full = (\n",
    "    full_model\n",
    "    .transform(df_full)\n",
    "    .withColumn(\"label\", col(\"Arrest\").cast(\"int\"))\n",
    "    .select(F.col(\"pcaFeatures\").alias(\"features\"), \"label\")\n",
    ")\n",
    "# Gravação do parquet “completo”\n",
    "ready_full.write.mode(\"overwrite\").parquet(READY_FULL)\n",
    "print(f\"✅ parquet completo pronto em {READY_FULL}  ({ready_full.count()} linhas)\")\n",
    "\n",
    "# ────────────────────────── 5) Gerar parquet pequeno balanceado (“READY_SMALL”) ─────────\n",
    "print(\"⏳ Preparar parquet pequeno balanceado …\")\n",
    "small_src = spark.read.parquet(INPUT_SMALL)\n",
    "\n",
    "# assegurar colunas temporais no pequeno também\n",
    "small_src = (\n",
    "    small_src\n",
    "    .withColumn(\"Date\", to_timestamp(\"Date\", \"MM/dd/yyyy HH:mm:ss a\"))\n",
    "    .filter(col(\"Date\").isNotNull())\n",
    "    .withColumn(\"Hour\",      hour(\"Date\"))\n",
    "    .withColumn(\"DayOfWeek\", dayofweek(\"Date\"))\n",
    ")\n",
    "\n",
    "# balanceio simples 50/50 – pega 50% dos positivos e 50% dos negativos\n",
    "pos = small_src.filter(\"Arrest = true\").sample(False, 0.5, 42)\n",
    "neg = small_src.filter(\"Arrest = false\").sample(False, 0.5, 42)\n",
    "df_small = pos.unionByName(neg)\n",
    "\n",
    "ready_small = (\n",
    "    full_model\n",
    "    .transform(df_small)\n",
    "    .withColumn(\"label\", col(\"Arrest\").cast(\"int\"))\n",
    "    .select(F.col(\"pcaFeatures\"), \"label\")\n",
    ")\n",
    "\n",
    "ready_small.write.mode(\"overwrite\").parquet(READY_SMALL)\n",
    "print(f\"✅ parquet pequeno salvo em {READY_SMALL} ({ready_small.count()} linhas)\")\n",
    "\n",
    "print(\"⌛ Elapsed:\", round(time.time() - t0, 1), \"s\")\n",
    "spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vscode_pyspark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
